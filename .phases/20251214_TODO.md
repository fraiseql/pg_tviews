# Benchmark Fixes TODO - December 14, 2025

## Executive Summary

**Status**: Ready for systematic debugging
**Risk Level**: Medium (well-isolated issues, clear fix paths)
**Estimated Completion**: 2-3 hours (Phase 1 only)

**Critical Insights**:
1. **Data Generation Error**: Psql variable interpolation syntax mismatch (`:var` vs `:'var'`)
2. **TVIEW Conversion Error**: Architectural limitation - PostgreSQL event triggers can't use SPI for catalog queries
3. **Fix Strategy**: Quick fixes available for both issues (no major refactoring needed)

**Key Architectural Decision Required**:
- **TVIEW Auto-Conversion**: Must disable in event triggers due to PostgreSQL SPI transaction limitations
- **Impact**: Users will manually call `pg_tviews_convert_existing_table()` after CREATE TABLE AS SELECT
- **Future**: Design background worker for automatic conversion (Phase 2/next sprint)

---

## Previous Work

**Previous Session**: See `.phases/completed/20251213_TODO_COMPLETED.md`

**Completed Yesterday (2025-12-13)**:
- ‚úÖ Fixed schema context (SET search_path)
- ‚úÖ Fixed benchmark infrastructure (schema-qualified function calls)
- ‚úÖ Fixed TVIEW validation (Rust - only require id + data)
- ‚úÖ Added id column to tv_product schema
- ‚úÖ Committed all fixes (commit 96054e4)

---

## üî¥ CRITICAL - Remaining Issues

### 1. Data Generation Error - Psql Variable Interpolation
**Status**: NOT FIXED ‚ùå
**Priority**: HIGH
**Risk**: BLOCKS all benchmarking
**Estimated Debug Time**: 30-60 minutes

**Error Evidence**:
```
psql:data/01_ecommerce_data.sql:151: ERROR: syntax error at or near ":"
```

**Architecture Context**:
- Psql variable substitution syntax: `:variable` (unquoted) vs `:'variable'` (quoted)
- Shell ‚Üí Docker ‚Üí psql variable passing chain
- Error at line 151 suggests issue downstream of variable declaration (line 21)

**Root Cause Hypothesis (Ranked by Likelihood)**:

1. **MOST LIKELY**: Mismatched psql variable syntax
   - Shell passes: `-v data_scale="small"` (correct)
   - SQL expects: `:'data_scale'` (quoted interpolation)
   - **Issue**: If variable value already contains quotes, psql double-quotes
   - **Test**: Line 151 likely uses `:data_scale` instead of `:'data_scale'`

2. **LIKELY**: Variable not propagated through Docker
   - PSQL variable may not cross container boundary
   - **Test**: Add `echo` before psql call to verify variable value

3. **UNLIKELY**: Line number mismatch (error reporting bug)
   - Psql error lines are generally accurate
   - **Test**: Add line numbers to SQL file for verification

4. **UNLIKELY**: Search path interference
   - Search path affects schema, not variable interpolation
   - **Dismiss**: Not relevant to this error

**Systematic Investigation Plan**:

```bash
# STEP 1: Verify variable at shell level (outside container)
cd test/sql/comprehensive_benchmarks
scale="small"
echo "Testing with scale: $scale"

# STEP 2: Test direct psql (bypass Docker)
psql -d pg_tviews_benchmark -v data_scale="$scale" \
  -c "DO \$\$ BEGIN RAISE NOTICE 'data_scale = %', :'data_scale'; END \$\$;"
# Expected: NOTICE: data_scale = small

# STEP 3: Inspect line 151 in data file
sed -n '145,155p' data/01_ecommerce_data.sql | cat -n
# Look for: Unquoted :data_scale or incorrect interpolation syntax

# STEP 4: Test with explicit value (eliminate variable issue)
psql -d pg_tviews_benchmark -f data/01_ecommerce_data.sql
# If this works, variable passing is the issue

# STEP 5: Full diagnostic run
./run_benchmarks.sh --scale small 2>&1 | tee /tmp/benchmark_debug.log
grep -A5 -B5 "ERROR" /tmp/benchmark_debug.log
```

**Fix Strategy** (based on hypothesis #1):
```sql
-- WRONG (line 151 likely has):
WHERE some_condition = :data_scale  -- Unquoted - expects integer/boolean

-- CORRECT (should be):
WHERE some_condition = :'data_scale'  -- Quoted - treats as string
```

**Verification**:
```bash
# After fix, verify variable interpolation:
psql -d pg_tviews_benchmark -v data_scale="small" \
  -c "SELECT :'data_scale' AS scale_value;"
# Expected: scale_value = small

# Then run full data generation:
psql -d pg_tviews_benchmark -v data_scale="small" \
  -f data/01_ecommerce_data.sql
```

---

### 2. TVIEW Conversion SPI Error - Transaction Context Issue
**Status**: NOT FIXED ‚ùå
**Priority**: HIGH
**Risk**: BLOCKS automatic TVIEW creation
**Estimated Debug Time**: 1-2 hours

**Error Evidence**:
```
INFO:  Converting existing table 'tv_product' to TVIEW
ERROR:  Failed to convert table to TVIEW: SPI query failed: SPI error: Transaction
Query: Unknown
```

**Architecture Context**:
- **Event Trigger Context**: DDL event triggers run in special transaction context
- **SPI Limitations**: Server Programming Interface has restrictions during DDL events
- **Transaction Visibility**: Metadata changes may not be visible to SPI during event
- **Critical Insight**: "Query: Unknown" suggests SPI_execute never received query

**Root Cause Hypothesis (Ranked by Likelihood)**:

1. **MOST LIKELY**: Event trigger SPI transaction isolation
   - **Problem**: Event triggers can't use SPI to query catalogs during DDL event
   - **Reason**: PostgreSQL prevents nested transactions in event triggers
   - **Evidence**: Error occurs during `CREATE TABLE AS SELECT` event
   - **Pattern**: This is a known PostgreSQL limitation for ddl_command_end triggers

2. **LIKELY**: Incorrect SPI connection management
   - **Problem**: SPI_connect/SPI_finish mismatch or nested calls
   - **Test**: Check if event_trigger.rs properly manages SPI lifecycle
   - **Evidence**: "Transaction" error suggests SPI connection state issue

3. **POSSIBLE**: Schema-qualified table name resolution
   - **Problem**: tv_product exists in 'benchmark' schema, not search_path
   - **Test**: Check if conversion function uses schema-qualified names
   - **Evidence**: Other schema context fixes were needed

4. **UNLIKELY**: Missing permissions
   - **Dismiss**: Extension functions run as superuser during CREATE EXTENSION

**Architectural Anti-Pattern Identified**:
```
‚ùå WRONG PATTERN: Event Trigger ‚Üí SPI Query ‚Üí Metadata Catalog
- Event triggers can't safely query catalogs during DDL events
- SPI transaction state conflicts with outer DDL transaction

‚úÖ CORRECT PATTERN: Event Trigger ‚Üí Deferred Action ‚Üí Background Worker
- Queue conversion request in event trigger (minimal work)
- Process conversion in separate transaction context
- Use background worker or NOTIFY/LISTEN pattern
```

**Systematic Investigation Plan**:

```bash
# STEP 1: Isolate event trigger from manual function call
psql -d pg_tviews_benchmark <<EOF
-- Test manual conversion (outside event trigger context)
SELECT pg_tviews_convert_existing_table('benchmark.tv_product');
EOF
# Expected: Success = SPI works outside events
#          Failure = SPI code itself broken

# STEP 2: Check SPI error details in Rust
# File: src/event_trigger.rs
# Look for: SPI::connect() calls, error handling, transaction management

# STEP 3: Test with explicit schema qualification
psql -d pg_tviews_benchmark <<EOF
SET search_path TO benchmark, public;
SELECT pg_tviews_convert_existing_table('tv_product');
EOF

# STEP 4: Review PostgreSQL event trigger limitations
# Documentation: https://www.postgresql.org/docs/current/event-trigger-definition.html
# Known limit: Event triggers cannot use SPI_execute for catalog queries

# STEP 5: Check SPI connection state
# Add logging to event_trigger.rs:
# elog::info!("SPI connected: {}", SPI::get_current() != null);
```

**Fix Strategy Options**:

**Option A: Disable Event Trigger Conversion (Quick Fix)**
```rust
// In src/event_trigger.rs - comment out automatic conversion
// Event trigger should ONLY validate, not convert
pub fn on_create_table_as_select_end() {
    // Validate TVIEW structure
    validate_tview_structure()?;

    // DO NOT auto-convert - let users call manually
    // convert_to_tview()?;  // ‚ùå Remove this

    Ok(())
}
```
**Impact**: Users must manually call `pg_tviews_convert_existing_table()`
**Pros**: Simple, avoids SPI transaction issues
**Cons**: Not automatic (acceptable for now)

**Option B: Deferred Conversion via NOTIFY (Proper Fix)**
```rust
// Event trigger queues conversion request
pub fn on_create_table_as_select_end() {
    let table_name = get_table_name()?;

    // Queue for background processing
    SPI::execute(&format!(
        "NOTIFY tview_conversion_queue, '{}'",
        table_name
    ))?;

    Ok(())
}

// Separate background worker processes queue
// (Requires background worker registration)
```
**Impact**: Requires background worker infrastructure
**Pros**: Proper async pattern, no transaction conflicts
**Cons**: More complex, needs worker setup

**Option C: Post-Transaction Hook (Medium Complexity)**
```rust
// Use PostgreSQL's register_xact_callback
// Convert after DDL transaction commits
// (Requires PG callback registration API)
```

**Recommended Approach**:
1. **Immediate**: Implement Option A (disable auto-convert)
2. **Next Sprint**: Design Option B (background worker)
3. **Document**: Manual conversion workflow for users

**Verification**:
```bash
# Test manual conversion workflow
psql -d pg_tviews_benchmark <<EOF
-- Create TVIEW (event trigger validates only)
CREATE TABLE tv_test AS SELECT id, data FROM some_view;

-- Manually convert to TVIEW
SELECT pg_tviews_convert_existing_table('tv_test');

-- Verify TVIEW created
SELECT * FROM pg_tviews_metadata WHERE table_name = 'tv_test';
EOF
```

**Documentation Required**:
- Update README: "Event triggers validate but don't auto-convert"
- Add example: Manual conversion workflow
- Explain limitation: SPI restrictions in event triggers
- Future roadmap: Background worker for automatic conversion

---

### 3. Benchmark Scenarios Psql Variable Quoting
**File**: `test/sql/comprehensive_benchmarks/run_benchmarks.sh:131`
**Status**: NOT FIXED ‚ùå
**Priority**: MEDIUM

**Issue**: Scenarios file uses incorrect quoting (though data gen is fixed)

**Current Code**:
```bash
# Line 131 (scenarios)
$PSQL -v data_scale="'$scale'" -f "scenarios/${scenario}_benchmarks.sql"
```

**Should Be**:
```bash
# Match data generation pattern (line 127)
$PSQL -v data_scale="$scale" -f "scenarios/${scenario}_benchmarks.sql"
```

**Fix Required**:
```bash
# In run_benchmarks.sh, change line 131 to:
$PSQL -v data_scale="$scale" -f "scenarios/${scenario}_benchmarks.sql"
```

**Impact**: Scenarios may fail with same psql variable error

---

## üü° MEDIUM PRIORITY - Verification Needed

### 4. End-to-End Benchmark Verification
**Status**: NOT TESTED
**Priority**: MEDIUM

**Objective**: Verify all fixes work together in full benchmark run

**Test Plan**:
1. Fix remaining data generation issue (#1)
2. Fix TVIEW conversion issue (#2)
3. Run full benchmark: `./scripts/master.sh --scale small`
4. Verify success criteria:
   - Schema loads without errors
   - Data generation completes
   - TVIEW conversion succeeds
   - Benchmarks execute
   - Results recorded

**Success Criteria**:
- [ ] Schema loads in benchmark schema (not public)
- [ ] Data generation completes for all scales
- [ ] tv_product TVIEW created successfully
- [ ] All benchmark tests execute
- [ ] Results written to benchmark_results table
- [ ] CSV export succeeds
- [ ] No "relation does not exist" errors

---

## üü¢ LOW PRIORITY - Enhancements

### 5. Documentation Updates
**Status**: PENDING
**Priority**: LOW

**Tasks**:
- Update QUICKSTART.md with correct psql variable quoting
- Document new TVIEW validation requirements (id + data only)
- Add examples of optimization columns (fk_*, path, *_id)
- Update benchmark architecture docs

---

### 6. Add Diagnostic Logging
**Status**: PENDING
**Priority**: LOW

**Improvements**:
```bash
# After schema load, add to run_benchmarks.sh:
log "  Verifying schema state..."
$PSQL -c "SELECT schemaname, tablename FROM pg_tables WHERE schemaname IN ('benchmark', 'public') ORDER BY schemaname, tablename;"

# After data generation:
log "  Verifying data loaded..."
$PSQL -c "SELECT 'tb_category' as table, COUNT(*) FROM benchmark.tb_category
          UNION ALL SELECT 'tb_product', COUNT(*) FROM benchmark.tb_product;"
```

---

## üìã Implementation Plan

### Phase 1: Debug & Fix (HIGH PRIORITY)
**Estimated Time**: 2-3 hours
**Risk Level**: Medium (well-isolated issues)

**Dependency Graph**:
```
Issue #1 (Data Gen) ‚îÄ‚îÄ‚îê
                      ‚îú‚îÄ‚îÄ> Issue #4 (E2E Test)
Issue #2 (TVIEW)   ‚îÄ‚îÄ‚îò

Issue #3 (Scenarios) ‚îÄ‚îÄ> Issue #4 (E2E Test)
```

**Execution Strategy**: Parallel debugging + sequential verification

---

#### Task 1.1: Data Generation Psql Variable Fix
**Status**: ‚ùå NOT STARTED
**Priority**: P0 (blocks all data loading)
**Time Estimate**: 30-60 min
**Dependencies**: None
**Risk**: Low (well-understood psql mechanics)

**Execution Steps**:
1. Read `data/01_ecommerce_data.sql:145-155` to identify line 151
2. Search for `:data_scale` (unquoted) usage
3. Replace with `:'data_scale'` (quoted) for string interpolation
4. Test with STEP 1-5 from investigation plan above
5. Verify data loads successfully

**Success Criteria**:
- [ ] Psql variable interpolation works
- [ ] Data generation completes without errors
- [ ] All tables populated (tb_category, tb_product)

**Rollback**: Git checkout if fix breaks other scenarios

---

#### Task 1.2: TVIEW Conversion Architecture Decision
**Status**: ‚ùå NOT STARTED
**Priority**: P0 (blocks automatic TVIEW creation)
**Time Estimate**: 1-2 hours
**Dependencies**: None (can run parallel with 1.1)
**Risk**: Medium (architectural change)

**Decision Point**: Choose fix strategy

**Phase 1.2a: Investigation (30 min)**
1. Test manual conversion: `SELECT pg_tviews_convert_existing_table('benchmark.tv_product');`
2. If SUCCESS ‚Üí Event trigger is the problem (proceed to 1.2b)
3. If FAILURE ‚Üí SPI code itself broken (deeper debug needed)

**Phase 1.2b: Quick Fix Implementation (30 min)**
- Implement **Option A**: Disable auto-conversion in event trigger
- Update `src/event_trigger.rs` to validate only
- Rebuild Docker image: `docker build -t pg_tviews .`
- Test manually: `CREATE TABLE tv_test AS ...; SELECT pg_tviews_convert_existing_table('tv_test');`

**Phase 1.2c: Documentation (30 min)**
- Update README with manual conversion workflow
- Add troubleshooting section for SPI limitations
- Document future roadmap (background worker)

**Success Criteria**:
- [ ] Manual conversion works: `SELECT pg_tviews_convert_existing_table(...)`
- [ ] Event trigger validates structure without crashing
- [ ] Users know to call conversion manually
- [ ] Architectural limitation documented

**Rollback**: Revert event_trigger.rs, rebuild image

---

#### Task 1.3: Scenarios Variable Fix
**Status**: ‚ùå NOT STARTED
**Priority**: P1 (blocks scenario benchmarks only)
**Time Estimate**: 5 min
**Dependencies**: None
**Risk**: Minimal (simple code change)

**Execution Steps**:
1. Edit `run_benchmarks.sh:131`
2. Change: `$PSQL -v data_scale="'$scale'"` ‚Üí `$PSQL -v data_scale="$scale"`
3. Test: `./run_benchmarks.sh --scale small` (after Task 1.1 completes)

**Success Criteria**:
- [ ] Scenarios execute without variable errors
- [ ] Consistent quoting with data generation (line 127)

**Rollback**: Git checkout (trivial change)

---

#### Task 1.4: End-to-End Verification
**Status**: ‚ùå NOT STARTED
**Priority**: P0 (validates all fixes)
**Time Estimate**: 30 min
**Dependencies**: Tasks 1.1, 1.2, 1.3 MUST complete first
**Risk**: Low (read-only verification)

**Execution Steps**:
```bash
# Clean slate
docker compose down -v
docker compose up -d

# Run full benchmark suite
cd test/sql/comprehensive_benchmarks
./run_benchmarks.sh --scale small 2>&1 | tee /tmp/benchmark_verification.log

# Check results
grep -i "error" /tmp/benchmark_verification.log
grep -i "success" /tmp/benchmark_verification.log

# Verify data
psql -d pg_tviews_benchmark -c "SELECT schemaname, tablename FROM pg_tables WHERE schemaname = 'benchmark';"
psql -d pg_tviews_benchmark -c "SELECT COUNT(*) FROM benchmark.tb_product;"
psql -d pg_tviews_benchmark -c "SELECT * FROM pg_tviews_metadata;"
```

**Success Criteria** (from TODO above):
- [ ] Schema loads in benchmark schema (not public)
- [ ] Data generation completes for all scales
- [ ] tv_product exists (even if not auto-converted to TVIEW)
- [ ] Manual TVIEW conversion works
- [ ] All benchmark tests execute
- [ ] Results written to benchmark_results table
- [ ] CSV export succeeds
- [ ] No "relation does not exist" errors

**Failure Handling**:
- If ANY criterion fails ‚Üí Debug specific issue
- If >2 criteria fail ‚Üí Rollback all changes, re-assess
- Document any new issues in fresh TODO

---

#### Task 1.5: Commit Verified Fixes
**Status**: ‚ùå NOT STARTED
**Priority**: P0 (preserve working state)
**Time Estimate**: 10 min
**Dependencies**: Task 1.4 MUST pass
**Risk**: None (git safety)

**Commit Strategy**:
```bash
# Separate commits for each fix (better git history)

# Commit 1: Data generation fix
git add test/sql/comprehensive_benchmarks/data/01_ecommerce_data.sql
git commit -m "fix(benchmarks): Fix psql variable interpolation in data generation

- Change :data_scale to :'data_scale' for string interpolation
- Fixes 'syntax error at or near :' at line 151
- Verified with manual psql test"

# Commit 2: TVIEW conversion architecture fix
git add src/event_trigger.rs
git commit -m "fix(tview): Disable auto-conversion in event trigger [ARCHITECTURE]

- Event triggers can't use SPI for catalog queries (PG limitation)
- Changed to validate-only mode
- Users must manually call pg_tviews_convert_existing_table()
- Documented in README with migration guide

Related: PostgreSQL SPI transaction isolation in ddl_command_end triggers"

# Commit 3: Scenarios variable fix
git add test/sql/comprehensive_benchmarks/run_benchmarks.sh
git commit -m "fix(benchmarks): Fix psql variable quoting in scenarios

- Match data generation pattern (line 127)
- Remove extra quotes: data_scale='$scale' ‚Üí data_scale=$scale
- Consistent variable passing across script"

# Commit 4: Documentation
git add README.md TROUBLESHOOTING.md
git commit -m "docs(tview): Add manual conversion workflow and SPI limitations

- Document event trigger validation-only behavior
- Add troubleshooting guide for SPI errors
- Include future roadmap for background worker conversion"
```

**Success Criteria**:
- [ ] Each fix in separate commit (clean history)
- [ ] Commit messages explain WHY, not just WHAT
- [ ] Architectural decisions documented in commit message
- [ ] All tests pass after each commit

### Phase 2: Enhancements (OPTIONAL)
**Estimated Time**: 1 hour

1. Update documentation
2. Add diagnostic logging
3. Create benchmark troubleshooting guide

---

## üöÄ Quick Start - Execution Order

**For rapid debugging, execute in this order:**

```bash
# 1. IMMEDIATE: Inspect the psql variable error (2 min)
cd test/sql/comprehensive_benchmarks
sed -n '145,155p' data/01_ecommerce_data.sql | grep -n "data_scale"
# Look for unquoted :data_scale ‚Üí Fix to :'data_scale'

# 2. QUICK WIN: Test manual TVIEW conversion (5 min)
psql -d pg_tviews_benchmark <<EOF
-- This tells us if SPI works outside event triggers
SELECT pg_tviews_convert_existing_table('benchmark.tv_product');
EOF
# Success = Only event trigger is broken (expected)
# Failure = Deeper SPI issue (unexpected)

# 3. PARALLEL: Fix data + scenarios (10 min)
# Edit data/01_ecommerce_data.sql line 151
# Edit run_benchmarks.sh line 131

# 4. REBUILD: If modifying event_trigger.rs (30 min)
docker build -t pg_tviews .
docker compose down -v
docker compose up -d

# 5. VERIFY: End-to-end test (20 min)
./run_benchmarks.sh --scale small 2>&1 | tee /tmp/verify.log
```

**Time to First Success**: 15-30 minutes (Tasks 1.1 + 1.3)
**Time to Full Fix**: 2-3 hours (includes architecture change + docs)

---

## üß™ Testing Strategy

### Hypothesis-Driven Testing

**Test Pyramid** (bottom-up approach):

```
Level 4: E2E Benchmark [30 min] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                 ‚îÇ
Level 3: Manual TVIEW Conversion [5 min] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                                                 ‚îú‚îÄ> Full Validation
Level 2: Psql Variable Interpolation [2 min] ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                                                 ‚îÇ
Level 1: File Inspection [2 min] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Level 1: Quick Diagnostic (No Database Needed)
```bash
# Inspect error location
cd test/sql/comprehensive_benchmarks
sed -n '145,155p' data/01_ecommerce_data.sql | cat -n

# Look for pattern mismatches:
grep -n ":data_scale" data/01_ecommerce_data.sql
# Unquoted = WRONG
# Quoted :'data_scale' = CORRECT
```

**Expected Outcome**: Identify exact line with incorrect syntax

---

### Level 2: Variable Interpolation Test (Isolated)
```bash
# Test psql variable passing (no data dependencies)
psql -d pg_tviews_benchmark -v data_scale="small" <<EOF
DO \$\$
BEGIN
    RAISE NOTICE 'Unquoted interpolation: %', :data_scale;  -- ERROR expected
    RAISE NOTICE 'Quoted interpolation: %', :'data_scale';  -- SUCCESS expected
END \$\$;
EOF
```

**Expected Outcome**:
- Unquoted `:data_scale` ‚Üí Syntax error (proves hypothesis)
- Quoted `:'data_scale'` ‚Üí Prints "small" (proves fix)

---

### Level 3: Manual Function Call Test (SPI Context)
```bash
# Test TVIEW conversion outside event trigger
psql -d pg_tviews_benchmark <<EOF
-- Ensure table exists first
SET search_path TO benchmark, public;
\dt tv_product

-- Test manual conversion
SELECT pg_tviews_convert_existing_table('benchmark.tv_product');

-- Verify TVIEW metadata
SELECT * FROM pg_tviews_metadata WHERE table_name = 'tv_product';
EOF
```

**Expected Outcomes**:
- **SUCCESS**: SPI works fine outside event triggers ‚Üí Event trigger is the issue (EXPECTED)
- **FAILURE**: SPI itself broken ‚Üí Deeper debugging needed (UNEXPECTED)

**Decision Tree**:
```
Manual conversion works?
‚îú‚îÄ YES ‚Üí Disable auto-conversion in event trigger (Option A)
‚îî‚îÄ NO  ‚Üí Debug SPI connection/transaction management
         ‚îú‚îÄ Check schema qualification
         ‚îú‚îÄ Review SPI::connect() lifecycle
         ‚îî‚îÄ Add detailed logging to convert.rs
```

---

### Level 4: Full Integration Test
```bash
# Clean environment
docker compose down -v
docker compose up -d

# Run complete benchmark
cd test/sql/comprehensive_benchmarks
./run_benchmarks.sh --scale small 2>&1 | tee /tmp/benchmark_verification.log

# Structured validation
echo "=== ERROR CHECK ==="
grep -i "error" /tmp/benchmark_verification.log | grep -v "0 errors"

echo "=== SUCCESS CHECK ==="
grep -i "success" /tmp/benchmark_verification.log

echo "=== SCHEMA VERIFICATION ==="
psql -d pg_tviews_benchmark <<EOF
SELECT schemaname, tablename, tableowner
FROM pg_tables
WHERE schemaname = 'benchmark'
ORDER BY tablename;
EOF

echo "=== DATA VERIFICATION ==="
psql -d pg_tviews_benchmark <<EOF
SELECT 'tb_category' as table, COUNT(*) FROM benchmark.tb_category
UNION ALL SELECT 'tb_product', COUNT(*) FROM benchmark.tb_product;
EOF

echo "=== TVIEW VERIFICATION ==="
psql -d pg_tviews_benchmark <<EOF
SELECT * FROM pg_tviews_metadata;
EOF
```

**Success Criteria Checklist**:
- [ ] Schema loads in 'benchmark' schema (not 'public')
- [ ] tb_category has >0 rows
- [ ] tb_product has >0 rows
- [ ] tv_product table exists (even if not TVIEW yet)
- [ ] Manual conversion succeeds: `SELECT pg_tviews_convert_existing_table(...)`
- [ ] pg_tviews_metadata has entry for tv_product after manual conversion
- [ ] Benchmark scenarios execute without errors
- [ ] Results written to benchmark_results table

---

### Observability & Debugging

**Add diagnostic logging** (if issues persist):

```bash
# In run_benchmarks.sh, after each major step:
log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*"; }

log "Loading schema..."
$PSQL -f schemas/01_ecommerce_schema.sql 2>&1 | tee -a $LOG_FILE

log "Verifying schema loaded..."
$PSQL -c "\dt benchmark.*" 2>&1 | tee -a $LOG_FILE

log "Generating data (scale=$scale)..."
$PSQL -v data_scale="$scale" -f data/01_ecommerce_data.sql 2>&1 | tee -a $LOG_FILE

log "Verifying data loaded..."
$PSQL -c "SELECT 'tb_product' as table, COUNT(*) FROM benchmark.tb_product;" 2>&1 | tee -a $LOG_FILE
```

**Rust-level debugging** (if SPI issue persists):

```rust
// In src/event_trigger.rs or src/ddl/convert.rs
use pgrx::prelude::*;

#[pg_guard]
pub fn pg_tviews_convert_existing_table(table_name: &str) -> Result<(), Box<dyn Error>> {
    elog!(INFO, "=== TVIEW CONVERSION DEBUG ===");
    elog!(INFO, "Table: {}", table_name);
    elog!(INFO, "SPI connected: {}", SPI::is_connected());

    // Existing conversion logic...

    elog!(INFO, "=== CONVERSION COMPLETE ===");
    Ok(())
}
```

---

## üõ°Ô∏è Risk Mitigation & Safety

### Pre-Flight Checklist
```bash
# BEFORE starting fixes:
- [ ] Commit current work: git add -A && git commit -m "WIP: before benchmark fixes"
- [ ] Create backup branch: git branch backup-20251214
- [ ] Verify Docker state: docker compose ps
- [ ] Document baseline: ./run_benchmarks.sh --scale small > /tmp/baseline.log 2>&1
- [ ] Check disk space: df -h (Docker volumes can grow large)
```

### Rollback Strategy

**If Task 1.1 (Data Generation) breaks things**:
```bash
git checkout test/sql/comprehensive_benchmarks/data/01_ecommerce_data.sql
# Restore original file, re-diagnose
```

**If Task 1.2 (TVIEW Conversion) breaks things**:
```bash
# Revert Rust code
git checkout src/event_trigger.rs

# Rebuild clean image
docker build --no-cache -t pg_tviews .
docker compose down -v
docker compose up -d
```

**If Task 1.3 (Scenarios) breaks things**:
```bash
git checkout test/sql/comprehensive_benchmarks/run_benchmarks.sh
```

**Nuclear option** (complete reset):
```bash
git checkout dev  # or your main branch
git reset --hard origin/dev
docker compose down -v
docker system prune -af --volumes  # WARNING: Deletes all Docker data
docker compose up -d
```

### Safety Guardrails

**DO**:
- ‚úÖ Test each fix independently
- ‚úÖ Commit after each successful fix (atomic commits)
- ‚úÖ Run verification after each change
- ‚úÖ Document architectural decisions in commit messages
- ‚úÖ Keep fixes minimal (no scope creep)

**DO NOT**:
- ‚ùå Combine multiple fixes in one commit
- ‚ùå Skip verification steps
- ‚ùå Make "while I'm here" improvements
- ‚ùå Modify Rust code without rebuilding Docker image
- ‚ùå Push to main/master without full verification

---

## üèóÔ∏è Architectural Patterns & Anti-Patterns

### Pattern: PostgreSQL Event Trigger Limitations

**Context**: Event triggers run during DDL command processing

**Anti-Pattern** ‚ùå:
```rust
// Event trigger trying to query catalogs via SPI
pub fn on_create_table_as_select_end() {
    let table_oid = SPI::execute("SELECT oid FROM pg_class WHERE ...")?;
    // ERROR: SPI transaction conflict
}
```

**Why it fails**:
- Event triggers run in the same transaction as the DDL command
- SPI calls create sub-transactions
- PostgreSQL prevents nested transactions during DDL events
- Metadata may not be visible until outer transaction commits

**Correct Pattern** ‚úÖ:
```rust
// Event trigger does minimal validation only
pub fn on_create_table_as_select_end() {
    // Only validate structure (no SPI needed)
    validate_columns_exist()?;

    // Log for manual follow-up
    elog!(INFO, "Table created. Run: SELECT pg_tviews_convert_existing_table('...');");

    Ok(())
}

// Separate function for manual conversion (called outside event context)
pub fn pg_tviews_convert_existing_table(table_name: &str) {
    // This runs in user transaction, SPI works fine
    SPI::connect(|| {
        let metadata = SPI::execute("SELECT oid FROM pg_class ...")?;
        // Process conversion
        Ok(())
    })
}
```

**Alternative Patterns** (future consideration):

1. **Background Worker**:
   ```rust
   // Event trigger queues work
   pub fn on_create_table_as_select_end() {
       enqueue_conversion_request(table_name)?;
   }

   // Background worker processes queue
   // (Runs in separate process, independent transaction)
   ```

2. **Post-Transaction Callback**:
   ```rust
   // Register callback to run after DDL commits
   register_xact_callback(XactEvent::COMMIT, || {
       convert_to_tview()?;
   });
   ```

3. **NOTIFY/LISTEN Pattern**:
   ```sql
   -- Event trigger sends notification
   NOTIFY tview_conversion_queue, 'benchmark.tv_product';

   -- External listener processes conversions
   -- (Daemon or pg_cron job)
   ```

### Pattern: Psql Variable Interpolation

**Context**: Shell ‚Üí Docker ‚Üí psql variable passing

**Anti-Pattern** ‚ùå:
```bash
# Shell script (wrong quoting)
psql -v data_scale="'$scale'" -f script.sql

# SQL file (unquoted interpolation)
WHERE condition = :data_scale  -- Expects integer, gets string
```

**Correct Pattern** ‚úÖ:
```bash
# Shell script (no extra quotes)
psql -v data_scale="$scale" -f script.sql

# SQL file (quoted interpolation for strings)
WHERE condition = :'data_scale'  -- Treats as string literal
```

**Variable Interpolation Rules**:
- `:variable` ‚Üí Unquoted (for numbers, booleans, SQL keywords)
- `:'variable'` ‚Üí Single-quoted (for strings, identifiers)
- `:"variable"` ‚Üí Double-quoted (for case-sensitive identifiers)

**Examples**:
```sql
-- Shell: psql -v scale=1000 -v table=products -v enable=true

-- Unquoted (numbers/booleans)
SELECT * FROM foo LIMIT :scale;           -- LIMIT 1000
SELECT * FROM foo WHERE active = :enable; -- WHERE active = true

-- Single-quoted (strings)
SELECT * FROM foo WHERE name = :'table';  -- WHERE name = 'products'

-- Double-quoted (identifiers)
SELECT * FROM :"table";                   -- SELECT * FROM "products"
```

### Pattern: Schema-Qualified Names in Multi-Schema Setup

**Context**: Benchmark uses 'benchmark' schema, not 'public'

**Anti-Pattern** ‚ùå:
```sql
-- Relying on search_path
SELECT pg_tviews_convert_existing_table('tv_product');
-- May fail if search_path doesn't include 'benchmark'
```

**Correct Pattern** ‚úÖ:
```sql
-- Always schema-qualify in scripts
SELECT pg_tviews_convert_existing_table('benchmark.tv_product');

-- Or set search_path explicitly
SET search_path TO benchmark, public;
SELECT pg_tviews_convert_existing_table('tv_product');
```

**Best Practice**: Use schema-qualified names in all benchmark SQL files

---

## üìù Notes

### Key Learnings from Yesterday

1. **Docker Build Timing**: Changes to Rust code require full rebuild - image build happens BEFORE runtime
2. **Validation Success**: New TVIEW validation (id + data only) successfully compiled and deployed
3. **Schema Context**: SET search_path critical for multi-schema isolation
4. **Psql Variables**: Quote handling is subtle - `:''variable'` double-quotes if variable already quoted

### Architecture Decisions

**TVIEW Pattern** (as of commit 96054e4):
```sql
-- REQUIRED (minimum)
CREATE TABLE tv_entity AS SELECT
    id,              -- UUID (required)
    data             -- JSONB (required)
FROM v_entity;

-- OPTIMIZED (recommended)
CREATE TABLE tv_entity AS SELECT
    id,              -- UUID (required)
    pk_entity,       -- INTEGER primary key (optimization)
    fk_parent,       -- INTEGER foreign key (filtering)
    parent_id,       -- UUID foreign key (joins)
    path,            -- LTREE (hierarchical queries)
    data             -- JSONB (required)
FROM v_entity;
```

**All additional columns are now valid and encouraged for performance!**

---

## üîó Related Files

**Completed Work**:
- `.phases/completed/20251213_TODO_COMPLETED.md` - Yesterday's completed tasks

**Code Changed** (commit 96054e4):
- `src/ddl/convert.rs` - TVIEW validation (Rust)
- `src/hooks.rs` - SELECT validation (Rust)
- `test/sql/comprehensive_benchmarks/schemas/01_ecommerce_schema.sql` - Added id column, SET search_path
- `test/sql/comprehensive_benchmarks/data/01_ecommerce_data.sql` - SET search_path
- `test/sql/comprehensive_benchmarks/scenarios/*.sql` - Schema-qualified function calls (7 files)

**Files to Investigate**:
- `test/sql/comprehensive_benchmarks/run_benchmarks.sh` - Shell script orchestration
- `src/event_trigger.rs` - TVIEW conversion logic
- `src/ddl/convert.rs` - Manual conversion function

---

## üìä Progress Tracking

**Overall Status**: 4/9 tasks completed (44%)

**Completed**:
- [x] Schema context fix
- [x] Function call schema qualification
- [x] TVIEW validation update
- [x] Add id column to schema

**Remaining**:
- [ ] Data generation error
- [ ] TVIEW conversion error
- [ ] Scenarios variable fix
- [ ] End-to-end verification
- [ ] Documentation updates

---

## üìä Priority Matrix & Effort Estimation

| Issue | Priority | Risk | Effort | Dependencies | Quick Win? |
|-------|----------|------|--------|--------------|------------|
| **#1 Data Generation** | P0 | LOW | 30-60 min | None | ‚úÖ YES |
| **#2 TVIEW Conversion** | P0 | MED | 1-2 hrs | None | ‚ö†Ô∏è Partial |
| **#3 Scenarios Variable** | P1 | MIN | 5 min | None | ‚úÖ YES |
| **#4 E2E Verification** | P0 | LOW | 30 min | #1, #2, #3 | ‚ùå NO |

**Recommended Execution Order**:
1. **Start with #1 + #3** (parallel, 30-60 min total) ‚Üí Immediate wins
2. **Then #2** (investigation + fix, 1-2 hrs) ‚Üí Architectural work
3. **Finally #4** (verification, 30 min) ‚Üí Validate everything

**Critical Path**: #1 ‚Üí #4 (blocks all testing)
**Parallel Path**: #3 can be done anytime

---

## üéØ Success Metrics

**Phase 1 Complete When**:
- [ ] Data generation works for all scales (small, medium, large)
- [ ] Manual TVIEW conversion succeeds
- [ ] Benchmark scenarios execute without errors
- [ ] All 4 commits pushed with clean history
- [ ] Documentation updated

**Phase 2 Complete When** (optional):
- [ ] Diagnostic logging added
- [ ] Troubleshooting guide written
- [ ] Architecture decisions documented

---

Last Updated: 2025-12-14
Status: Ready for systematic debugging (hypothesis-driven approach)
Next Session: Execute Quick Start commands ‚Üí Level 1-2 tests first
Estimated Time to Working State: 2-3 hours
